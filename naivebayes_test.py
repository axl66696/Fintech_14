# -*- coding: utf-8 -*-
"""NaiveBayes_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TdG9vymZh9iWGZZ2hw9gVSVQOXxbSO69

#下載處理好的資料
"""

f1id = '1kMA6Ddp04-hnEihtg3Hu1ElvJh__pOBJ'
f2id = '1tgPcn9Aebq6rjSmVHZb7cQy6YWlld0da'
f3id = '1w5Cpt3jHW8z979K-hsLIemoDz6iqb9Z9'

!pip3 install gdown
import gdown
url = 'https://drive.google.com/uc?id=%s'%(f1id)
output = 'train_sorted_strConverted_fillNA.csv'
gdown.download(url, output, quiet=False)

url2 = 'https://drive.google.com/uc?id=%s'%(f2id)
output2 = 'train_locdt_loctm_converted.csv'
gdown.download(url2, output2, quiet=False)

url3 = 'https://drive.google.com/uc?id=%s'%(f3id)
output3 = 'train_X_feature_normalization_onehot.csv'
gdown.download(url3, output3, quiet=False)

"""#讀入資料"""

import pandas as pd
df = pd.read_csv('train_sorted_strConverted_fillNA.csv')
df_loct = pd.read_csv('train_locdt_loctm_converted.csv')
df_X_feature = pd.read_csv('train_X_feature_normalization_onehot.csv')

df['fraud_ind'].value_counts()[1]

df_loct['fraud_ind'].value_counts()[1]

df_X_feature['fraud_ind'].value_counts()[1]

"""#使用的資料

完全用原本的，使用df

將類別balance
"""

df_2 = df.sample(frac=1, random_state=42)

fraud_df = df_2.loc[df_2['fraud_ind'] == 1]
non_fraud_df = df_2.loc[df_2['fraud_ind'] == 0][:20355]

normal_distributed_df = pd.concat([fraud_df, non_fraud_df])

df_balance = normal_distributed_df.sample(frac=1, random_state=42)
# new_df
df_balance.sort_values(by=['locdt','loctm'], inplace=True)
df_balance

"""把data和time轉換，使用df_loct"""

# df_2_loct = df_loct.sample(frac=1, random_state=42)

# fraud_df_loct = df_2_loct.loc[df_2['fraud_ind'] == 1]
# non_fraud_df_loct = df_2_loct.loc[df_2['fraud_ind'] == 0][:20355]

# normal_distributed_df_loct = pd.concat([fraud_df_loct, non_fraud_df_loct])

# new_df_loct = normal_distributed_df_loct.sample(frac=1, random_state=42)
# # new_df
# new_df_loct.sort_values(by='locdt', inplace=True)
# new_df_loct

"""用300多個feature，用df_X_feature

#拆分資料
"""

y = df['fraud_ind']
x = df.drop('fraud_ind', axis = 1)

y_balance = df_balance['fraud_ind']
x_balance = df_balance.drop('fraud_ind', axis = 1)

y_loct = df_loct['fraud_ind']
x_loct = df_loct.drop('fraud_ind', axis = 1)

y_X_feature = df_X_feature['fraud_ind']
x_X_feature = df_X_feature.drop('fraud_ind', axis = 1)

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, shuffle=False) #train與test資料
x_train_part, x_val, y_train_part, y_val = train_test_split(x_train, y_train, test_size=0.2, shuffle=False) #train與val資料

x_balance_train, x_balance_test, y_balance_train, y_balance_test = train_test_split(x_balance, y_balance, test_size=0.2, shuffle=False) #train與test資料
x_balance_train_part, x_balance_val, y_balance_train_part, y_balance_val = train_test_split(x_balance_train, y_balance_train, test_size=0.2, shuffle=False) #train與val資料

x_loct_train, x_loct_test, y_loct_train, y_loct_test = train_test_split(x_loct, y_loct, test_size=0.2, shuffle=False) #train與test資料
x_loct_train_part, x_loct_val, y_loct_train_part, y_loct_val = train_test_split(x_loct_train, y_loct_train, test_size=0.2, shuffle=False) #train與val資料

x_X_feature_train, x_X_feature_test, y_X_feature_train, y_X_feature_test = train_test_split(x_X_feature, y_X_feature, test_size=0.2, shuffle=False) #train與test資料
x_X_feature_train_part, x_X_feature_val, y_X_feature_train_part, y_X_feature_val = train_test_split(x_X_feature_train, y_X_feature_train, test_size=0.2, shuffle=False) #train與val資料

"""#開始訓練"""

# from sklearn.datasets import make_regression
# from sklearn.ensemble import RandomForestRegressor
# import matplotlib.pyplot as plt 

# model = RandomForestRegressor()
# # fit the model
# model.fit(x_train_part, y_train_part)

# # get importance
# importance = model.feature_importances_

# # summarize feature importance
# f=[] #feature index list
# col = x.columns 
# for i,v in enumerate(importance):
#   print("Feature{:<2d}: {:<15s}, Score: {:.5f}".format(i,col[i] ,v))
#   f.append(i)

# col = x.columns
# # plot feature importance
# plt.rcParams["figure.figsize"] = (25, 8)
# plt.bar([x for x in range(len(importance))], importance)
# plt.xticks(f,col)
# plt.show()

# x_train_rf = x_train[x_train.columns[[6,19,16,18,1]]]
# x_val_rf = x_val[x_val.columns[[6,19,16,18,1]]]

from sklearn.naive_bayes import GaussianNB

NB = GaussianNB()
NB.fit(x_train_part, y_train_part)
y_pred = NB.predict(x_test)

NB_balance = GaussianNB()
NB_balance.fit(x_balance_train_part, y_balance_train_part)
y_balance_pred = NB_balance.predict(x_test)

NB_loct = GaussianNB()
NB_loct.fit(x_loct_train_part, y_loct_train_part)
y_loct_pred = NB_loct.predict(x_loct_test)

NB_X_feature = GaussianNB()
NB_X_feature.fit(x_X_feature_train_part, y_X_feature_train_part)
y_X_feature_pred = NB_X_feature.predict(x_X_feature_test)

"""# Metrics"""

from sklearn.metrics import roc_auc_score
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score

"""Naive Bayes

ROC分數
"""

roc_auc_score(y_test, y_pred)

roc_auc_score(y_test, y_balance_pred)

roc_auc_score(y_loct_test, y_loct_pred)

roc_auc_score(y_X_feature_test, y_X_feature_pred)

"""F1-score分數"""

th = 0.6  #設定threshold

f1_score(y_test,(y_pred > th).astype(int))

f1_score(y_test,(y_balance_pred > th).astype(int))

f1_score(y_loct_test,(y_loct_pred > th).astype(int))

f1_score(y_X_feature_test,(y_X_feature_pred > th).astype(int))

"""Accuracy分數"""

print('Start predicting...')

accuracy = accuracy_score(y_test,(y_pred > th).astype(int))*100
print ("Accuracy of XGBOOST is: ", accuracy)

print('Start predicting...')

accuracy = accuracy_score(y_test,(y_balance_pred > th).astype(int))*100
print ("Accuracy of XGBOOST is: ", accuracy)

print('Start predicting...')

accuracy = accuracy_score(y_loct_test,(y_loct_pred > th).astype(int))*100
print ("Accuracy of XGBOOST is: ", accuracy)

print('Start predicting...')

accuracy = accuracy_score(y_X_feature_test,(y_X_feature_pred > th).astype(int))*100
print ("Accuracy of XGBOOST is: ", accuracy)

"""precision"""

from sklearn.metrics import precision_score
from sklearn.metrics import recall_score

precision = precision_score(y_test, (y_pred > th).astype(int), zero_division=0)
print("precision : ", precision)

precision = precision_score(y_test, (y_balance_pred > th).astype(int), zero_division=0)
print("precision : ", precision)

precision = precision_score(y_loct_test, (y_loct_pred > th).astype(int), zero_division=0)
print("precision : ", precision)

precision = precision_score(y_X_feature_test, (y_X_feature_pred > th).astype(int), zero_division=0)
print("precision : ", precision)

"""recall"""

recall = recall_score(y_test, (y_pred > th).astype(int))
print("recall : ",recall)

recall = recall_score(y_test, (y_balance_pred > th).astype(int))
print("recall : ",recall)

recall = recall_score(y_loct_test, (y_loct_pred > th).astype(int))
print("recall : ",recall)

recall = recall_score(y_X_feature_test, (y_X_feature_pred > th).astype(int))
print("recall : ",recall)