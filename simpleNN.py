# -*- coding: utf-8 -*-
"""simpleNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n3LFoFWTBeFH2c2gHTBEB3jrDTKcoCgx
"""

from google.colab import drive
drive.mount('/content/drive')

from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score , average_precision_score 
from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve ,auc , log_loss ,  classification_report 
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np

"""# Data Processing scenario 1
- fill null data and convert str to int
- 64%(0.8 * 0.8) data to train
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 0
"""

import pandas as pd
with open('/content/drive/My Drive/fintech-introduction/term-project/train_sorted_strConverted_fillNA.csv', 'r') as f:
  df = pd.read_csv(f)
  # df = df.set_index('txkey')


# print(df_X)
# print(df_y)
df

df_y = df['fraud_ind']
df_x = df.drop(['fraud_ind', 'txkey'], 1)
df_x

# split train+val and test
df_x_data, df_x_test = train_test_split(df_x, test_size=0.2, shuffle=False)
df_y_data = df_y.loc[df_x_data.index]
df_y_test = df_y.loc[df_x_test.index]

# print(df_x_data)
# print(df_x_test)
# print(df_y_data)
# print(df_y_test)

#split train and val
df_x_train, df_x_val = train_test_split(df_x_data, test_size=0.2, shuffle=False)
df_y_train = df_y_data.loc[df_x_train.index]
df_y_val = df_y_data.loc[df_x_val.index]


print('df_x_train:')
print(df_x_train)
print('df_x_val:')
print(df_x_val)
print('df_x_test:')
print(df_x_test)
print('df_y_train:')
print(df_y_train)
print('df_y_val:')
print(df_y_val)
print('df_y_test:')
print(df_y_test)

import torch.nn as nn
import torch
from torch.utils.data import TensorDataset, DataLoader
import math
device = torch.device("cuda:0" if torch.cuda.is_available() else 'cpu')

lr = 1e-2
batch_size = 973943
# batch_num = math.ceil(df_x_train.shape[0] / batch_size)
max_epoch = 100

x_train = torch.from_numpy(df_x_train.values).to(device)
y_train = torch.from_numpy(df_y_train.values).to(device)
loader = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)

x_val = torch.from_numpy(df_x_val.values).to(device)
y_val = torch.from_numpy(df_y_val.values).to(device)

x_test = torch.from_numpy(df_x_test.values).to(device)
y_test = torch.from_numpy(df_y_test.values).to(device)

# n = x_train.shape[1]
n = 21
class simpleNN(torch.nn.Module):
    def __init__(self):
        super(simpleNN, self).__init__()
        self.layers = nn.Sequential(
            # nn.BatchNorm1d(n),
            nn.Linear(n, 10),
            nn.BatchNorm1d(10),
            nn.Sigmoid(),
            nn.Linear(10,5),
            nn.BatchNorm1d(5),
            nn.Sigmoid(),
            nn.Linear(5,2),
            # nn.BatchNorm1d(2),
            nn.Sigmoid(),
        )

    def forward(self, x):
        self.y_pred = self.layers(x.float())
        return self.y_pred

loss_func = nn.CrossEntropyLoss().to(device)
model = simpleNN()
model = model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

"""### Training"""

model.train()
val_f1_max = 0
for epoch in range(max_epoch):
  e_loss = 0
  e_acc = 0
  e_prc = 0
  e_rec = 0
  e_f1 = 0

  for x_batch, y_batch in loader:
    current_size = y_batch.shape[0]
    optimizer.zero_grad()
    y_pred = model(x_batch)
    y_scalar = []
    for d in y_pred:
      if d[0]>d[1]:
        y_scalar.append(0)
      else:
        y_scalar.append(1)
    
    loss = loss_func(y_pred.double(), y_batch.flatten().long()).to(device)
    e_loss += loss.item()
    # loss = loss.float()

    e_acc += accuracy_score(y_batch, y_scalar)
    e_prc += precision_score(y_batch, y_scalar)
    e_rec += recall_score(y_batch, y_scalar)
    e_f1 += f1_score(y_batch, y_scalar)

    loss.backward()
    optimizer.step()
  
  e_acc = (e_acc / batch_num)
  e_loss = (e_loss / batch_num)
  e_prc = (e_prc / batch_num)
  e_rec = (e_rec / batch_num)
  e_f1 = (e_f1 / batch_num)
  print('epoch: {}, loss: {}, acc: {}, prc: {}, rec: {}, f1: {}'.format(epoch, e_loss, e_acc, e_prc, e_rec, e_f1))

  if epoch%10 == 0:
    y_pred = model(x_val)
    y_scalar = []
    for d in y_pred:
      if d[0]>d[1]:
        y_scalar.append(0)
      else:
        y_scalar.append(1)
    

    val_loss = loss_func(y_pred.double(), y_val.flatten().long()).to(device)
    val_acc = accuracy_score(y_val, y_scalar)
    val_prc = precision_score(y_val, y_scalar)
    val_rec = recall_score(y_val, y_scalar)
    val_f1 = f1_score(y_val, y_scalar)
    print("================validation================")
    print('epoch: {}, loss: {}, acc: {}, prc: {}, rec: {}, f1: {}'.format(epoch, val_loss, val_acc, val_prc, val_rec, val_f1))
    if val_f1 > val_f1_max:
        val_f1_max = val_f1
        torch.save(model.state_dict(), '/content/drive/My Drive/fintech-introduction/term-project/simpleNN_s1.pkl')
        print('update_file')
    print('==========================================')


torch.cuda.empty_cache()

"""### Show the performance on testing set"""

model_test = simpleNN()
model_test.load_state_dict(torch.load('/content/drive/My Drive/fintech-introduction/term-project/simpleNN_s1.pkl'))
model_test = model_test.to(device)

y_pred = model_test(x_test)
y_scalar = []
for d in y_pred:
  if d[0]>d[1]:
    y_scalar.append(0)
  else:
    y_scalar.append(1)

y_target = y_test
cm = confusion_matrix(y_target, y_scalar)
# row: actually, column: predict, the smaller class will be the target for counting the following indexies
print(cm)

# these libraries will automatically take the fewer class to evaluate the performance
print('accuracy_score: ', accuracy_score(y_target, y_scalar))
print("precision_score", precision_score(y_target, y_scalar))
print("recall_score",    recall_score(y_target, y_scalar))
print("f1_score",        f1_score(y_target, y_scalar))

print(model)

"""# Data Processing scenario 2
- Base on scenerio 1
- Balance two class
- Sample 40000 data in 64%(0.8 * 0.8) training data
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 0.13
"""

# Balance two #example in two class
# df_y_train
df_fraud = df_x_train.loc[df_y_train == 1]
df_non_fraud = df_x_train.loc[df_y_train == 0].sample(n=df_fraud.shape[0])
df_x_train_balance = pd.concat([df_fraud, df_non_fraud])
df_y_train_balance = pd.concat([df_y_train.loc[df_fraud.index], df_y_train.loc[df_non_fraud.index]])
df_fraud, df_non_fraud, df_x_train_balance, df_y_train_balance

import torch.nn as nn
import torch
from torch.utils.data import TensorDataset, DataLoader
import math
device = torch.device("cuda:0" if torch.cuda.is_available() else 'cpu')

lr = 1e-2
batch_size = 29578
batch_num = math.ceil(df_x_train_balance.shape[0] / batch_size)
max_epoch = 100

x_train = torch.from_numpy(df_x_train_balance.values).to(device)
y_train = torch.from_numpy(df_y_train_balance.values).to(device)
loader = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)

n = x_train.shape[1]
class simpleNN(torch.nn.Module):
    def __init__(self):
        super(simpleNN, self).__init__()
        self.conv = nn.Sequential(
            nn.BatchNorm1d(n),
            nn.Linear(n, 10),
            nn.BatchNorm1d(10),
            nn.Sigmoid(),
            nn.Linear(10,5),
            nn.BatchNorm1d(5),
            nn.Sigmoid(),
            nn.Linear(5,2),
            # nn.BatchNorm1d(2),
            nn.Sigmoid(),
        )

    def forward(self, x):
        self.y_pred = self.conv(x.float())
        return self.y_pred

loss_func = nn.CrossEntropyLoss().to(device)
model = simpleNN()
model = model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

model.train()
val_f1_max = 0
for epoch in range(max_epoch):
  e_loss = 0
  e_acc = 0
  e_prc = 0
  e_rec = 0
  e_f1 = 0

  for x_batch, y_batch in loader:
    current_size = y_batch.shape[0]
    optimizer.zero_grad()
    y_pred = model(x_batch)
    y_scalar = []
    for d in y_pred:
      if d[0]>d[1]:
        y_scalar.append(0)
      else:
        y_scalar.append(1)
    
    loss = loss_func(y_pred.double(), y_batch.flatten().long()).to(device)
    e_loss += loss.item()
    # loss = loss.float()

    e_acc += accuracy_score(y_batch, y_scalar)
    e_prc += precision_score(y_batch, y_scalar)
    e_rec += recall_score(y_batch, y_scalar)
    e_f1 += f1_score(y_batch, y_scalar)

    loss.backward()
    optimizer.step()
  
  e_acc = (e_acc / batch_num)
  e_loss = (e_loss / batch_num)
  e_prc = (e_prc / batch_num)
  e_rec = (e_rec / batch_num)
  e_f1 = (e_f1 / batch_num)
  print('epoch: {}, loss: {}, acc: {}, prc: {}, rec: {}, f1: {}'.format(epoch, e_loss, e_acc, e_prc, e_rec, e_f1))

  if epoch%10 == 0:
    y_pred = model(x_val)
    y_scalar = []
    for d in y_pred:
      if d[0]>d[1]:
        y_scalar.append(0)
      else:
        y_scalar.append(1)
    

    val_loss = loss_func(y_pred.double(), y_val.flatten().long()).to(device)
    val_acc = accuracy_score(y_val, y_scalar)
    val_prc = precision_score(y_val, y_scalar)
    val_rec = recall_score(y_val, y_scalar)
    val_f1 = f1_score(y_val, y_scalar)
    print("================validation================")
    print('epoch: {}, loss: {}, acc: {}, prc: {}, rec: {}, f1: {}'.format(epoch, val_loss, val_acc, val_prc, val_rec, val_f1))
    if val_f1 > val_f1_max:
        val_f1_max = val_f1
        torch.save(model.state_dict(), '/content/drive/My Drive/fintech-introduction/term-project/simpleNN_s2.pkl')
        print('update_file')
    print('==========================================')


torch.cuda.empty_cache()

"""### Show the performance on testing set"""

model_test = simpleNN()
model_test.load_state_dict(torch.load('/content/drive/My Drive/fintech-introduction/term-project/simpleNN_s2.pkl'))
model_test = model_test.to(device)
model_test.eval()

y_pred = model_test(x_test)
y_scalar = []
for d in y_pred:
  if d[0]>d[1]:
    y_scalar.append(0)
  else:
    y_scalar.append(1)

y_target = y_test
cm = confusion_matrix(y_target, y_scalar)
# row: actually, column: predict, the smaller class will be the target for counting the following indexies
print(cm)

# these libraries will automatically take the fewer class to evaluate the performance
print('accuracy_score: ', accuracy_score(y_target, y_scalar))
print("precision_score", precision_score(y_target, y_scalar))
print("recall_score",    recall_score(y_target, y_scalar))
print("f1_score",        f1_score(y_target, y_scalar))

print(model)

"""# Data Processing scenario 3
- Base on scenerio 1
- Convert locdt and loctm
- 64%(0.8 * 0.8) training data
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 0.17
"""

import pandas as pd
with open('/content/drive/My Drive/fintech-introduction/term-project/train_locdt_loctm_converted.csv', 'r') as f:
  df = pd.read_csv(f)
  # df = df.set_index('txkey')


# print(df_X)
# print(df_y)
df

df_y = df['fraud_ind']
df_x = df.drop(['fraud_ind', 'txkey'], 1)
df_x

# split train+val and test
df_x_data, df_x_test = train_test_split(df_x, test_size=0.2, shuffle=False)
df_y_data = df_y.loc[df_x_data.index]
df_y_test = df_y.loc[df_x_test.index]

# print(df_x_data)
# print(df_x_test)
# print(df_y_data)
# print(df_y_test)

#split train and val
df_x_train, df_x_val = train_test_split(df_x_data, test_size=0.2, shuffle=False)
df_y_train = df_y_data.loc[df_x_train.index]
df_y_val = df_y_data.loc[df_x_val.index]


print('df_x_train:')
print(df_x_train)
print('df_x_val:')
print(df_x_val)
print('df_x_test:')
print(df_x_test)
print('df_y_train:')
print(df_y_train)
print('df_y_val:')
print(df_y_val)
print('df_y_test:')
print(df_y_test)

import torch.nn as nn
import torch
from torch.utils.data import TensorDataset, DataLoader
import math
device = torch.device("cuda:0" if torch.cuda.is_available() else 'cpu')

lr = 1e-2
batch_size = 973943
batch_num = math.ceil(df_x_train.shape[0] / batch_size)
max_epoch = 100

x_train = torch.from_numpy(df_x_train.values).to(device)
y_train = torch.from_numpy(df_y_train.values).to(device)
loader = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)

x_val = torch.from_numpy(df_x_val.values).to(device)
y_val = torch.from_numpy(df_y_val.values).to(device)

x_test = torch.from_numpy(df_x_test.values).to(device)
y_test = torch.from_numpy(df_y_test.values).to(device)

n = x_train.shape[1]
class simpleNN(torch.nn.Module):
    def __init__(self):
        super(simpleNN, self).__init__()
        self.conv = nn.Sequential(
            # nn.BatchNorm1d(n),
            nn.Linear(n, 10),
            nn.BatchNorm1d(10),
            nn.Sigmoid(),
            nn.Linear(10,5),
            nn.BatchNorm1d(5),
            nn.Sigmoid(),
            nn.Linear(5,2),
            # nn.BatchNorm1d(2),
            nn.Sigmoid(),
        )

    def forward(self, x):
        self.y_pred = self.conv(x.float())
        return self.y_pred

loss_func = nn.CrossEntropyLoss().to(device)
model = simpleNN()
model = model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

"""### Training"""

model.train()
val_f1_max = 0
for epoch in range(max_epoch):
  e_loss = 0
  e_acc = 0
  e_prc = 0
  e_rec = 0
  e_f1 = 0

  for x_batch, y_batch in loader:
    current_size = y_batch.shape[0]
    optimizer.zero_grad()
    y_pred = model(x_batch)
    y_scalar = []
    for d in y_pred:
      if d[0]>d[1]:
        y_scalar.append(0)
      else:
        y_scalar.append(1)
    
    loss = loss_func(y_pred.double(), y_batch.flatten().long()).to(device)
    e_loss += loss.item()
    # loss = loss.float()

    e_acc += accuracy_score(y_batch, y_scalar)
    e_prc += precision_score(y_batch, y_scalar)
    e_rec += recall_score(y_batch, y_scalar)
    e_f1 += f1_score(y_batch, y_scalar)

    loss.backward()
    optimizer.step()
  
  e_acc = (e_acc / batch_num)
  e_loss = (e_loss / batch_num)
  e_prc = (e_prc / batch_num)
  e_rec = (e_rec / batch_num)
  e_f1 = (e_f1 / batch_num)
  print('epoch: {}, loss: {}, acc: {}, prc: {}, rec: {}, f1: {}'.format(epoch, e_loss, e_acc, e_prc, e_rec, e_f1))

  if epoch%10 == 0:
    y_pred = model(x_val)
    y_scalar = []
    for d in y_pred:
      if d[0]>d[1]:
        y_scalar.append(0)
      else:
        y_scalar.append(1)
    

    val_loss = loss_func(y_pred.double(), y_val.flatten().long()).to(device)
    val_acc = accuracy_score(y_val, y_scalar)
    val_prc = precision_score(y_val, y_scalar)
    val_rec = recall_score(y_val, y_scalar)
    val_f1 = f1_score(y_val, y_scalar)
    print("================validation================")
    print('epoch: {}, loss: {}, acc: {}, prc: {}, rec: {}, f1: {}'.format(epoch, val_loss, val_acc, val_prc, val_rec, val_f1))
    if val_f1 > val_f1_max:
        val_f1_max = val_f1
        torch.save(model.state_dict(), '/content/drive/My Drive/fintech-introduction/term-project/simpleNN_s3.pkl')
        print('update_file')
    print('==========================================')


torch.cuda.empty_cache()

"""### Show the performance on testing set"""

model_test = simpleNN()
model_test.load_state_dict(torch.load('/content/drive/My Drive/fintech-introduction/term-project/simpleNN_s3.pkl'))
model_test = model_test.to(device)
model_test.eval()

y_pred = model_test(x_test)
y_scalar = []
for d in y_pred:
  if d[0]>d[1]:
    y_scalar.append(0)
  else:
    y_scalar.append(1)

y_target = y_test
cm = confusion_matrix(y_target, y_scalar)
# row: actually, column: predict, the smaller class will be the target for counting the following indexies
print(cm)

# these libraries will automatically take the fewer class to evaluate the performance
print('accuracy_score: ', accuracy_score(y_target, y_scalar))
print("precision_score", precision_score(y_target, y_scalar))
print("recall_score",    recall_score(y_target, y_scalar))
print("f1_score",        f1_score(y_target, y_scalar))

print(model)

"""# Data Processing scenario 4
- Base on scenerio 3
- Do normalization on numerical features
- Convert catogorical featrues to one hot style
- Only about 400000 data in total
- 64%(0.8 * 0.8) training data
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 0.2
"""

import pandas as pd
with open('/content/drive/My Drive/fintech-introduction/term-project/train_feature_normalization_onehot.csv', 'r') as f:
  df = pd.read_csv(f)
  # df = df.set_index('txkey')


# print(df_X)
# print(df_y)
df

df_y = df['fraud_ind']
df_x = df.drop(['fraud_ind', 'Unnamed: 0'], 1)
df_x

# split train+val and test
df_x_data, df_x_test = train_test_split(df_x, test_size=0.2, shuffle=False)
df_y_data = df_y.loc[df_x_data.index]
df_y_test = df_y.loc[df_x_test.index]

# print(df_x_data)
# print(df_x_test)
# print(df_y_data)
# print(df_y_test)

#split train and val
df_x_train, df_x_val = train_test_split(df_x_data, test_size=0.2, shuffle=False)
df_y_train = df_y_data.loc[df_x_train.index]
df_y_val = df_y_data.loc[df_x_val.index]


print('df_x_train:')
print(df_x_train)
print('df_x_val:')
print(df_x_val)
print('df_x_test:')
print(df_x_test)
print('df_y_train:')
print(df_y_train)
print('df_y_val:')
print(df_y_val)
print('df_y_test:')
print(df_y_test)

import torch.nn as nn
import torch
from torch.utils.data import TensorDataset, DataLoader
import math
device = torch.device("cuda:0" if torch.cuda.is_available() else 'cpu')

lr = 1e-2
batch_size = 292182
batch_num = math.ceil(df_x_train.shape[0] / batch_size)
max_epoch = 100

x_train = torch.from_numpy(df_x_train.values).to(device)
y_train = torch.from_numpy(df_y_train.values).to(device)
loader = DataLoader(TensorDataset(x_train, y_train), batch_size=batch_size, shuffle=True)

x_val = torch.from_numpy(df_x_val.values).to(device)
y_val = torch.from_numpy(df_y_val.values).to(device)

x_test = torch.from_numpy(df_x_test.values).to(device)
y_test = torch.from_numpy(df_y_test.values).to(device)

n = x_train.shape[1]
class simpleNN(torch.nn.Module):
    def __init__(self):
        super(simpleNN, self).__init__()
        self.conv = nn.Sequential(
            nn.Linear(n, 10),
            nn.BatchNorm1d(10),
            nn.Sigmoid(),
            nn.Linear(10,5),
            nn.BatchNorm1d(5),
            nn.Sigmoid(),
            nn.Linear(5,2),
            # nn.BatchNorm1d(2),
            nn.Sigmoid(),
        )

    def forward(self, x):
        self.y_pred = self.conv(x.float())
        return self.y_pred

loss_func = nn.CrossEntropyLoss().to(device)
model = simpleNN()
model = model.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

"""### Training"""

model.train()
val_f1_max = 0
for epoch in range(max_epoch):
  e_loss = 0
  e_acc = 0
  e_prc = 0
  e_rec = 0
  e_f1 = 0

  for x_batch, y_batch in loader:
    current_size = y_batch.shape[0]
    optimizer.zero_grad()
    y_pred = model(x_batch)
    y_scalar = []
    for d in y_pred:
      if d[0]>d[1]:
        y_scalar.append(0)
      else:
        y_scalar.append(1)
    
    loss = loss_func(y_pred.double(), y_batch.flatten().long()).to(device)
    e_loss += loss.item()
    # loss = loss.float()

    e_acc += accuracy_score(y_batch, y_scalar)
    e_prc += precision_score(y_batch, y_scalar)
    e_rec += recall_score(y_batch, y_scalar)
    e_f1 += f1_score(y_batch, y_scalar)

    loss.backward()
    optimizer.step()
  
  e_acc = (e_acc / batch_num)
  e_loss = (e_loss / batch_num)
  e_prc = (e_prc / batch_num)
  e_rec = (e_rec / batch_num)
  e_f1 = (e_f1 / batch_num)
  print('epoch: {}, loss: {}, acc: {}, prc: {}, rec: {}, f1: {}'.format(epoch, e_loss, e_acc, e_prc, e_rec, e_f1))

  if epoch%10 == 0:
    y_pred = model(x_val)
    y_scalar = []
    for d in y_pred:
      if d[0]>d[1]:
        y_scalar.append(0)
      else:
        y_scalar.append(1)
    

    val_loss = loss_func(y_pred.double(), y_val.flatten().long()).to(device)
    val_acc = accuracy_score(y_val, y_scalar)
    val_prc = precision_score(y_val, y_scalar)
    val_rec = recall_score(y_val, y_scalar)
    val_f1 = f1_score(y_val, y_scalar)
    print("================validation================")
    print('epoch: {}, loss: {}, acc: {}, prc: {}, rec: {}, f1: {}'.format(epoch, val_loss, val_acc, val_prc, val_rec, val_f1))
    if val_f1 > val_f1_max:
        val_f1_max = val_f1
        torch.save(model.state_dict(), '/content/drive/My Drive/fintech-introduction/term-project/simpleNN_s4.pkl')
        print('update_file')
    print('==========================================')


torch.cuda.empty_cache()

"""### Show the performance on testing set"""

model_test = simpleNN()
model_test.load_state_dict(torch.load('/content/drive/My Drive/fintech-introduction/term-project/simpleNN_s4.pkl'))
model_test = model_test.to(device)
model_test.eval()

y_pred = model_test(x_test)
y_scalar = []
for d in y_pred:
  if d[0]>d[1]:
    y_scalar.append(0)
  else:
    y_scalar.append(1)

y_target = y_test
cm = confusion_matrix(y_target, y_scalar)
# row: actually, column: predict, the smaller class will be the target for counting the following indexies
print(cm)

# these libraries will automatically take the fewer class to evaluate the performance
print('accuracy_score: ', accuracy_score(y_target, y_scalar))
print("precision_score", precision_score(y_target, y_scalar))
print("recall_score",    recall_score(y_target, y_scalar))
print("f1_score",        f1_score(y_target, y_scalar))

print(model)