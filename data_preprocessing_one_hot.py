# -*- coding: utf-8 -*-
"""data_preprocessing_one_hot

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o-R-XCpptssOfC-P4x0gZ8nUDtLaDIm9

# 酨入資料
"""

# 如果有使用 coloab 再執行此 cell
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

"""# Load Data and Overview"""

with open('/content/drive/My Drive/fintech-introduction/term-project/train_locdt_loctm_converted.csv', 'r') as f:
  df = pd.read_csv(f)
  df_y = df['fraud_ind']
  df = df.drop('fraud_ind', 1)
df

df_y

df.keys()

# 欄位類別判定
col_names_cont = [] # 數值型資料
col_names_disc = [] # 類別型資料
col_has_na = [] # 待補NA資料
for c in df.keys():
    uni = df[c].unique()
    n_na = pd.isna(df[c]).sum() # NA 數量
    if n_na > 0:
        col_has_na.append(c)       
    if len(uni) < 200 :
        print(f"{c}: uni={uni}")
        col_names_disc.append(c)
    else:
        info = [ df[c].max(), df[c].min(), df[c].mean(), df[c].std()]
        info = [ round(x,2) for x in info ]
        offset = 1 if n_na else 0
        diversity = (len(uni)-offset)/(len(df)-n_na)
        print(f"{c}: dtype={df[c].dtype}, n_na={n_na}")
        print("       max={}, min={}, mean={}, std={}, diversity={:.2f}%".format(*info, diversity*100 ) )
        if diversity == 1.0:
            print(f"       Delete col [{c}] due to diversity is 100% ")
        else:
            col_names_cont.append(c)

"""- 看起來數值型資料都沒有缺失值(n_na==0)"""

print('col_names_cont: {}, sum: {}'.format(col_names_cont, len(col_names_cont)))
print('col_names_disc: {}, sum: {}'.format(col_names_disc, len(col_names_disc)))
print('col_has_na: {}, sum: {}'.format(col_has_na, len(col_has_na)))

"""# Data Preprocessing

## 數值型
"""

df_cont = df[col_names_cont].copy()
df_cont.head()

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_cont = scaler.fit_transform(df_cont)
X_cont.shape, type(X_cont)

df_cont = pd.DataFrame(data=X_cont, index=df.index, columns=col_names_cont)
df_cont.head()

df_cont.describe()

df_disc = df[col_names_disc].copy()
df = pd.concat((df_cont, df_disc), 1)
df = pd.concat((df, df_y), 1)
df

df.to_csv("/content/drive/My Drive/fintech-introduction/term-project/train_feature_normalization.csv")

"""## 類別型"""

df_disc = df[col_names_disc].copy()
df_disc.head()

df_disc.shape

from sklearn.preprocessing import LabelEncoder
les = {}
for c in col_names_disc:
    le = LabelEncoder()
    df_disc.loc[:,c] = le.fit_transform(df_disc.loc[:,c])
    les.update({c:le})
df_disc.head()

from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(sparse=False)
X_disc = ohe.fit_transform(df_disc)
# cut_point = ohe.feature_indices_
# print("feature cut point: ", cut_point)
X_disc.shape, type(X_disc)

new_col_names_disc = []
for c in col_names_disc: 
    le = les[c]
    new_col_names_disc += [ c+'_'+str(cl) for cl in le.classes_ ]
assert len(new_col_names_disc) == X_disc.shape[1]

df_disc = pd.DataFrame(data=X_disc, index=df.index, columns=new_col_names_disc)
df_disc.head()

df = pd.concat((df_cont, df_disc), 1)
df

df = pd.concat((df, df_y), 1)
df

df = df.sample(frac=0.3).sort_index()
df

df

df.to_csv("/content/drive/My Drive/fintech-introduction/term-project/train_feature_normalization_onehot.csv")

# with open('/content/drive/My Drive/fintech-introduction/term-project/train_X_feature_normalization_onehot.csv', 'r') as f:
#   df = pd.read_csv(f)

# df

# df.drop('Unnamed: 0', 1)

"""## 類別型
- 只選hh, locdt, stocn, hcefg, ecfg
"""

df_disc = df[col_names_disc].copy()
df_disc = df_disc[['hh', 'locdt', 'stocn', 'hcefg', 'ecfg']]
df_disc.head()

df_disc.shape

col_names_disc = ['hh', 'locdt', 'stocn', 'hcefg', 'ecfg']
from sklearn.preprocessing import LabelEncoder
les = {}
for c in col_names_disc:
    le = LabelEncoder()
    df_disc.loc[:,c] = le.fit_transform(df_disc.loc[:,c])
    les.update({c:le})
df_disc.head()

from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(sparse=False)
X_disc = ohe.fit_transform(df_disc)
# cut_point = ohe.feature_indices_
# print("feature cut point: ", cut_point)
X_disc.shape, type(X_disc)

new_col_names_disc = []
for c in col_names_disc: 
    le = les[c]
    new_col_names_disc += [ c+'_'+str(cl) for cl in le.classes_ ]
assert len(new_col_names_disc) == X_disc.shape[1]

df_disc = pd.DataFrame(data=X_disc, index=df.index, columns=new_col_names_disc)
df_disc.head()

df = pd.concat((df_cont, df_disc), 1)
df

df = pd.concat((df, df_y), 1)
df

df = df.sample(frac=0.5).sort_index()
df

df.to_csv("/content/drive/My Drive/fintech-introduction/term-project/train_feature_normalization_onehot_subjective.csv")

# with open('/content/drive/My Drive/fintech-introduction/term-project/train_X_feature_normalization_onehot.csv', 'r') as f:
#   df = pd.read_csv(f)

# df

# df.drop('Unnamed: 0', 1)