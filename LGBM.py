# -*- coding: utf-8 -*-
"""LGBM_Tony.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iBXwiJ1Addmj_XkBMnd8n3BOrYUYBbAF
"""

from google.colab import drive
drive.mount('/content/drive')

from sklearn.metrics import roc_auc_score, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score , average_precision_score 
from sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve ,auc , log_loss ,  classification_report 
import matplotlib.pyplot as plt
import lightgbm as lgb
from sklearn.model_selection import train_test_split
import numpy as np

"""# Data Processing scenario 1
- fill null data and convert str to int
- 64%(0.8 * 0.8) data to train
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 0.35
"""

import pandas as pd
with open('/content/drive/My Drive/fintech-introduction/term-project/train_sorted_strConverted_fillNA.csv', 'r') as f:
  df = pd.read_csv(f)
  # df = df.set_index('txkey')


# print(df_X)
# print(df_y)
df

df_y = df['fraud_ind']
df_x = df.drop(['fraud_ind', 'txkey'], 1)
df_x

# split train+val and test
df_x_data, df_x_test = train_test_split(df_x, test_size=0.2, shuffle=False)
df_y_data = df_y.loc[df_x_data.index]
df_y_test = df_y.loc[df_x_test.index]

# print(df_x_data)
# print(df_x_test)
# print(df_y_data)
# print(df_y_test)

#split train and val
df_x_train, df_x_val = train_test_split(df_x_data, test_size=0.2, shuffle=False)
df_y_train = df_y_data.loc[df_x_train.index]
df_y_val = df_y_data.loc[df_x_val.index]


print('df_x_train:')
print(df_x_train)
print('df_x_val:')
print(df_x_val)
print('df_x_test:')
print(df_x_test)
print('df_y_train:')
print(df_y_train)
print('df_y_val:')
print(df_y_val)
print('df_y_test:')
print(df_y_test)

df_y_test

# create dataset for lightgbm
lgb_train = lgb.Dataset(df_x_train, df_y_train)
lgb_eval = lgb.Dataset(df_x_val, df_y_val)
print(lgb_train.data)
print(lgb_eval.data)

"""### Training"""

# specify your configurations as a dict

lgb_param = {
    'boosting_type' : 'gbdt',
    'objective' : 'cross_entropy',
    'metric' : 'cross_entropy',
    'learning_rate' : 0.1,
}

print('Starting training...')
max_round = 100
gbm = lgb.train(lgb_param,
         lgb_train,
         num_boost_round=max_round,
         valid_sets=lgb_eval,
         early_stopping_rounds=max_round)

"""### Show the performance on testing set

"""

target_y = df_y_test
y_pred = gbm.predict(df_x_test, num_iteration=gbm.best_iteration)

# print(y_pred)
print('df_y_test.shape: {}'.format(df_y_test.shape))
print('auc: {}'.format(roc_auc_score(target_y, y_pred)))
th = 0.5

# row: actually, column: predict, the smaller class will be the target for counting the following indexies
cm = confusion_matrix(target_y, (y_pred > th).astype(int), labels=[0,1])
print(cm)
print('precision_score: {}'.format(precision_score(target_y, (y_pred > th).astype(int))))
print('recall_score: {}'.format(recall_score(target_y, (y_pred > th).astype(int))))
print('f1_score: {}'.format(f1_score(target_y, (y_pred > th).astype(int))))
print('accuracy: {}'.format(accuracy_score(target_y, (y_pred > th).astype(int))))

# json_model = gbm.dump_model()
# import pprint
# pprint.pprint(json_model)

"""# Data Processing scenario 2
- Base on scenerio 1
- Balance two class
- Sample 40000 data in 64%(0.8 * 0.8) training data
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 0.16
"""

# Balance two #example in two class
# df_y_train
df_fraud = df_x_train.loc[df_y_train == 1]
df_non_fraud = df_x_train.loc[df_y_train == 0].sample(n=df_fraud.shape[0])
df_x_train_balance = pd.concat([df_fraud, df_non_fraud])
df_y_train_balance = pd.concat([df_y_train.loc[df_fraud.index], df_y_train.loc[df_non_fraud.index]])
df_fraud, df_non_fraud, df_x_train_balance, df_y_train_balance

df_x_train_balance

# create dataset for lightgbm
lgb_train = lgb.Dataset(df_x_train_balance, df_y_train_balance)
lgb_eval = lgb.Dataset(df_x_val, df_y_val)
print(lgb_train.data)
print(lgb_eval.data)

"""### Training"""

# specify your configurations as a dict

lgb_param = {
    'boosting_type' : 'gbdt',
    'objective' : 'binary',
    'metric' : 'binary_logloss',
    'learning_rate' : 0.1,
}

print('Starting training...')
max_round = 100
gbm = lgb.train(lgb_param,
         lgb_train,
         num_boost_round=max_round,
         valid_sets=lgb_eval,
         early_stopping_rounds=max_round)

"""### Show the performance on testing set

"""

target_y = df_y_test
y_pred = gbm.predict(df_x_test, num_iteration=gbm.best_iteration)

# print(y_pred)
print('df_y_test.shape: {}'.format(df_y_test.shape))
print('auc: {}'.format(roc_auc_score(target_y, y_pred)))
th = 0.5

# row: actually, column: predict, the smaller class will be the target for counting the following indexies
cm = confusion_matrix(target_y, (y_pred > th).astype(int), labels=[0,1])
print(cm)
print('precision_score: {}'.format(precision_score(target_y, (y_pred > th).astype(int))))
print('recall_score: {}'.format(recall_score(target_y, (y_pred > th).astype(int))))
print('f1_score: {}'.format(f1_score(target_y, (y_pred > th).astype(int))))
print('accuracy: {}'.format(accuracy_score(target_y, (y_pred > th).astype(int))))

"""# Data Processing scenario 3
- Base on scenerio 1
- Convert locdt and loctm
- 64%(0.8 * 0.8) training data
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 0.35
"""

import pandas as pd
with open('/content/drive/My Drive/fintech-introduction/term-project/train_locdt_loctm_converted.csv', 'r') as f:
  df = pd.read_csv(f)
  # df = df.set_index('txkey')


# print(df_X)
# print(df_y)
df

df_y = df['fraud_ind']
df_x = df.drop(['fraud_ind', 'txkey'], 1)
df_x

# split train+val and test
df_x_data, df_x_test = train_test_split(df_x, test_size=0.2, shuffle=False)
df_y_data = df_y.loc[df_x_data.index]
df_y_test = df_y.loc[df_x_test.index]

# print(df_x_data)
# print(df_x_test)
# print(df_y_data)
# print(df_y_test)

#split train and val
df_x_train, df_x_val = train_test_split(df_x_data, test_size=0.2, shuffle=False)
df_y_train = df_y_data.loc[df_x_train.index]
df_y_val = df_y_data.loc[df_x_val.index]


print('df_x_train:')
print(df_x_train)
print('df_x_val:')
print(df_x_val)
print('df_x_test:')
print(df_x_test)
print('df_y_train:')
print(df_y_train)
print('df_y_val:')
print(df_y_val)
print('df_y_test:')
print(df_y_test)

# create dataset for lightgbm
lgb_train = lgb.Dataset(df_x_train, df_y_train)
lgb_eval = lgb.Dataset(df_x_val, df_y_val)
print(lgb_train.data)
print(lgb_eval.data)

"""### Training"""

# specify your configurations as a dict

lgb_param = {
    'boosting_type' : 'gbdt',
    'objective' : 'binary',
    'metric' : 'binary_logloss',
    'learning_rate' : 0.1,
}

print('Starting training...')
max_round = 100
gbm = lgb.train(lgb_param,
         lgb_train,
         num_boost_round=max_round,
         valid_sets=lgb_eval,
         early_stopping_rounds=max_round)

"""### Show the performance on testing set"""

target_y = df_y_test
y_pred = gbm.predict(df_x_test, num_iteration=gbm.best_iteration)

# print(y_pred)
print('df_y_test.shape: {}'.format(df_x_test.shape))
print('auc: {}'.format(roc_auc_score(target_y, y_pred)))
th = 0.5

# row: actually, column: predict, the smaller class will be the target for counting the following indexies
cm = confusion_matrix(target_y, (y_pred > th).astype(int), labels=[0,1])
print(cm)
print('precision_score: {}'.format(precision_score(target_y, (y_pred > th).astype(int))))
print('recall_score: {}'.format(recall_score(target_y, (y_pred > th).astype(int))))
print('f1_score: {}'.format(f1_score(target_y, (y_pred > th).astype(int))))
print('accuracy: {}'.format(accuracy_score(target_y, (y_pred > th).astype(int))))

"""# Data Processing scenario 4
- Base on scenerio 3
- Do normalization on numerical features
- Convert catogorical featrues to one hot style
- Only about 400000 data in total
- 64%(0.8 * 0.8) training data
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 0.32
"""

import pandas as pd
with open('/content/drive/My Drive/fintech-introduction/term-project/train_feature_normalization_onehot.csv', 'r') as f:
  df = pd.read_csv(f)
  # df = df.set_index('txkey')


# print(df_X)
# print(df_y)
df

df_y = df['fraud_ind']
df_x = df.drop(['fraud_ind', 'Unnamed: 0'], 1)
df_x

# split train+val and test
df_x_data, df_x_test = train_test_split(df_x, test_size=0.2, shuffle=False)
df_y_data = df_y.loc[df_x_data.index]
df_y_test = df_y.loc[df_x_test.index]

# print(df_x_data)
# print(df_x_test)
# print(df_y_data)
# print(df_y_test)

#split train and val
df_x_train, df_x_val = train_test_split(df_x_data, test_size=0.2, shuffle=False)
df_y_train = df_y_data.loc[df_x_train.index]
df_y_val = df_y_data.loc[df_x_val.index]


print('df_x_train:')
print(df_x_train)
print('df_x_val:')
print(df_x_val)
print('df_x_test:')
print(df_x_test)
print('df_y_train:')
print(df_y_train)
print('df_y_val:')
print(df_y_val)
print('df_y_test:')
print(df_y_test)

# create dataset for lightgbm
lgb_train = lgb.Dataset(df_x_train, df_y_train)
lgb_eval = lgb.Dataset(df_x_val, df_y_val)
print(lgb_train.data)
print(lgb_eval.data)

"""### Training"""

# specify your configurations as a dict

lgb_param = {
    'boosting_type' : 'gbdt',
    'objective' : 'binary',
    'metric' : 'binary_logloss',
    'learning_rate' : 0.01,
}

print('Starting training...')
max_round = 1000
gbm = lgb.train(lgb_param,
         lgb_train,
         num_boost_round=max_round,
         valid_sets=lgb_eval,
         early_stopping_rounds=max_round)

"""### Show the performance on testing set"""

target_y = df_y_test
y_pred = gbm.predict(df_x_test, num_iteration=gbm.best_iteration)

# print(y_pred)
print('df_y_test.shape: {}'.format(df_x_test.shape))
print('auc: {}'.format(roc_auc_score(target_y, y_pred)))
th = 0.5

# row: actually, column: predict, the smaller class will be the target for counting the following indexies
cm = confusion_matrix(target_y, (y_pred > th).astype(int), labels=[0,1])
print(cm)
print('precision_score: {}'.format(precision_score(target_y, (y_pred > th).astype(int))))
print('recall_score: {}'.format(recall_score(target_y, (y_pred > th).astype(int))))
print('f1_score: {}'.format(f1_score(target_y, (y_pred > th).astype(int))))
print('accuracy: {}'.format(accuracy_score(target_y, (y_pred > th).astype(int))))

"""# Data Processing scenario 5
- Base on scenerio 3
- Do normalization on numerical features
- 64%(0.8 * 0.8) training data
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 0.34
"""

import pandas as pd
with open('/content/drive/My Drive/fintech-introduction/term-project/train_feature_normalization.csv', 'r') as f:
  df = pd.read_csv(f)
  # df = df.set_index('txkey')


# print(df_X)
# print(df_y)
df

df_y = df['fraud_ind']
df_x = df.drop(['fraud_ind', 'Unnamed: 0'], 1)
df_x

# split train+val and test
df_x_data, df_x_test = train_test_split(df_x, test_size=0.2, shuffle=False)
df_y_data = df_y.loc[df_x_data.index]
df_y_test = df_y.loc[df_x_test.index]

# print(df_x_data)
# print(df_x_test)
# print(df_y_data)
# print(df_y_test)

#split train and val
df_x_train, df_x_val = train_test_split(df_x_data, test_size=0.2, shuffle=False)
df_y_train = df_y_data.loc[df_x_train.index]
df_y_val = df_y_data.loc[df_x_val.index]


print('df_x_train:')
print(df_x_train)
print('df_x_val:')
print(df_x_val)
print('df_x_test:')
print(df_x_test)
print('df_y_train:')
print(df_y_train)
print('df_y_val:')
print(df_y_val)
print('df_y_test:')
print(df_y_test)

# create dataset for lightgbm
lgb_train = lgb.Dataset(df_x_train, df_y_train)
lgb_eval = lgb.Dataset(df_x_val, df_y_val)
print(lgb_train.data)
print(lgb_eval.data)

"""### Training

"""

# specify your configurations as a dict

lgb_param = {
    'boosting_type' : 'gbdt',
    'objective' : 'binary',
    'metric' : 'binary_logloss',
    'learning_rate' : 0.1,
}

print('Starting training...')
max_round = 100
gbm = lgb.train(lgb_param,
         lgb_train,
         num_boost_round=max_round,
         valid_sets=lgb_eval,
         early_stopping_rounds=max_round)

"""### Show the performance on testing set"""

target_y = df_y_test
y_pred = gbm.predict(df_x_test, num_iteration=gbm.best_iteration)

# print(y_pred)
print('df_y_test.shape: {}'.format(df_x_test.shape))
print('auc: {}'.format(roc_auc_score(target_y, y_pred)))
th = 0.5

# row: actually, column: predict, the smaller class will be the target for counting the following indexies
cm = confusion_matrix(target_y, (y_pred > th).astype(int), labels=[0,1])
print(cm)
print('precision_score: {}'.format(precision_score(target_y, (y_pred > th).astype(int))))
print('recall_score: {}'.format(recall_score(target_y, (y_pred > th).astype(int))))
print('f1_score: {}'.format(f1_score(target_y, (y_pred > th).astype(int))))
print('accuracy: {}'.format(accuracy_score(target_y, (y_pred > th).astype(int))))

"""# Data Processing scenario 6
- Base on scenerio 1
- Only use the feature that selected by subjective
  - drop column : acquic, bacno, cano, mchno
- 64%(0.8 * 0.8) training data
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 0.34
"""

import pandas as pd
with open('/content/drive/My Drive/fintech-introduction/term-project/train_sorted_strConverted_fillNA.csv', 'r') as f:
  df = pd.read_csv(f)
  # df = df.set_index('txkey')


# print(df_X)
# print(df_y)
df

df_y = df['fraud_ind']
df_x = df.drop(['fraud_ind', 'txkey', 'acqic', 'bacno', 'cano', 'mchno'], 1)
df_x

# split train+val and test
df_x_data, df_x_test = train_test_split(df_x, test_size=0.2, shuffle=False)
df_y_data = df_y.loc[df_x_data.index]
df_y_test = df_y.loc[df_x_test.index]

# print(df_x_data)
# print(df_x_test)
# print(df_y_data)
# print(df_y_test)

#split train and val
df_x_train, df_x_val = train_test_split(df_x_data, test_size=0.2, shuffle=False)
df_y_train = df_y_data.loc[df_x_train.index]
df_y_val = df_y_data.loc[df_x_val.index]


print('df_x_train:')
print(df_x_train)
print('df_x_val:')
print(df_x_val)
print('df_x_test:')
print(df_x_test)
print('df_y_train:')
print(df_y_train)
print('df_y_val:')
print(df_y_val)
print('df_y_test:')
print(df_y_test)

# create dataset for lightgbm
lgb_train = lgb.Dataset(df_x_train, df_y_train)
lgb_eval = lgb.Dataset(df_x_val, df_y_val)
print(lgb_train.data)
print(lgb_eval.data)

"""### Training"""

# specify your configurations as a dict

lgb_param = {
    'boosting_type' : 'gbdt',
    'objective' : 'binary',
    'metric' : 'binary',
    'learning_rate' : 0.1,
}

print('Starting training...')
max_round = 100
gbm = lgb.train(lgb_param,
         lgb_train,
         num_boost_round=max_round,
         valid_sets=lgb_eval,
         early_stopping_rounds=max_round)

"""### Show the performance on testing set

"""

target_y = df_y_test
y_pred = gbm.predict(df_x_test, num_iteration=gbm.best_iteration)

# print(y_pred)
print('df_y_test.shape: {}'.format(df_x_test.shape))
print('auc: {}'.format(roc_auc_score(target_y, y_pred)))
th = 0.5

# row: actually, column: predict, the smaller class will be the target for counting the following indexies
cm = confusion_matrix(target_y, (y_pred > th).astype(int), labels=[0,1])
print(cm)
print('precision_score: {}'.format(precision_score(target_y, (y_pred > th).astype(int))))
print('recall_score: {}'.format(recall_score(target_y, (y_pred > th).astype(int))))
print('f1_score: {}'.format(f1_score(target_y, (y_pred > th).astype(int))))
print('accuracy: {}'.format(accuracy_score(target_y, (y_pred > th).astype(int))))

# json_model = gbm.dump_model()
# import pprint
# pprint.pprint(json_model)

"""# Data Processing scenario 7
- Base on scenerio 3
- Do normalization on numerical features
- Only use the feature that selected by subjective
  - drop column : acquic, bacno, cano, mchno
- 64%(0.8 * 0.8) training data
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 0.34
"""

import pandas as pd
with open('/content/drive/My Drive/fintech-introduction/term-project/train_feature_normalization.csv', 'r') as f:
  df = pd.read_csv(f)
  # df = df.set_index('txkey')


# print(df_X)
# print(df_y)
df

df_y = df['fraud_ind']
df_x = df.drop(['fraud_ind', 'Unnamed: 0', 'acqic', 'bacno', 'cano', 'mchno'], 1)
df_x

# split train+val and test
df_x_data, df_x_test = train_test_split(df_x, test_size=0.2, shuffle=False)
df_y_data = df_y.loc[df_x_data.index]
df_y_test = df_y.loc[df_x_test.index]

# print(df_x_data)
# print(df_x_test)
# print(df_y_data)
# print(df_y_test)

#split train and val
df_x_train, df_x_val = train_test_split(df_x_data, test_size=0.2, shuffle=False)
df_y_train = df_y_data.loc[df_x_train.index]
df_y_val = df_y_data.loc[df_x_val.index]


print('df_x_train:')
print(df_x_train)
print('df_x_val:')
print(df_x_val)
print('df_x_test:')
print(df_x_test)
print('df_y_train:')
print(df_y_train)
print('df_y_val:')
print(df_y_val)
print('df_y_test:')
print(df_y_test)

# create dataset for lightgbm
lgb_train = lgb.Dataset(df_x_train, df_y_train)
lgb_eval = lgb.Dataset(df_x_val, df_y_val)
print(lgb_train.data)
print(lgb_eval.data)

"""### Training

"""

# specify your configurations as a dict

lgb_param = {
    'boosting_type' : 'gbdt',
    'objective' : 'binary',
    'metric' : 'binary_logloss',
    'learning_rate' : 0.1,
}

print('Starting training...')
max_round = 100
gbm = lgb.train(lgb_param,
         lgb_train,
         num_boost_round=max_round,
         valid_sets=lgb_eval,
         early_stopping_rounds=max_round)

"""### Show the performance on testing set"""

target_y = df_y_test
y_pred = gbm.predict(df_x_test, num_iteration=gbm.best_iteration)

# print(y_pred)
print('df_y_test.shape: {}'.format(df_x_test.shape))
print('auc: {}'.format(roc_auc_score(target_y, y_pred)))
th = 0.5

# row: actually, column: predict, the smaller class will be the target for counting the following indexies
cm = confusion_matrix(target_y, (y_pred > th).astype(int), labels=[0,1])
print(cm)
print('precision_score: {}'.format(precision_score(target_y, (y_pred > th).astype(int))))
print('recall_score: {}'.format(recall_score(target_y, (y_pred > th).astype(int))))
print('f1_score: {}'.format(f1_score(target_y, (y_pred > th).astype(int))))
print('accuracy: {}'.format(accuracy_score(target_y, (y_pred > th).astype(int))))

"""# Data Processing scenario 8
- Base on scenerio 3
- Do normalization on numerical features
- Only use the feature that selected by subjective
  - use column : hh, mm, ss, locdt, stocn, hecfg, ecfg
- 64%(0.8 * 0.8) training data
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 
"""

import pandas as pd
with open('/content/drive/My Drive/fintech-introduction/term-project/train_feature_normalization.csv', 'r') as f:
  df = pd.read_csv(f)
  # df = df.set_index('txkey')


# print(df_X)
# print(df_y)
df

df_y = df['fraud_ind']
df_x = df[['hh']]
df_x

# split train+val and test
df_x_data, df_x_test = train_test_split(df_x, test_size=0.2, shuffle=False)
df_y_data = df_y.loc[df_x_data.index]
df_y_test = df_y.loc[df_x_test.index]

# print(df_x_data)
# print(df_x_test)
# print(df_y_data)
# print(df_y_test)

#split train and val
df_x_train, df_x_val = train_test_split(df_x_data, test_size=0.2, shuffle=False)
df_y_train = df_y_data.loc[df_x_train.index]
df_y_val = df_y_data.loc[df_x_val.index]


print('df_x_train:')
print(df_x_train)
print('df_x_val:')
print(df_x_val)
print('df_x_test:')
print(df_x_test)
print('df_y_train:')
print(df_y_train)
print('df_y_val:')
print(df_y_val)
print('df_y_test:')
print(df_y_test)

# create dataset for lightgbm
lgb_train = lgb.Dataset(df_x_train, df_y_train)
lgb_eval = lgb.Dataset(df_x_val, df_y_val)
print(lgb_train.data)
print(lgb_eval.data)

"""### Training

"""

# specify your configurations as a dict

lgb_param = {
    'boosting_type' : 'gbdt',
    'objective' : 'binary',
    'metric' : 'binary_logloss',
    'learning_rate' : 0.1,
}

print('Starting training...')
max_round = 100
gbm = lgb.train(lgb_param,
         lgb_train,
         num_boost_round=max_round,
         valid_sets=lgb_eval,
         early_stopping_rounds=max_round)

"""### Show the performance on testing set"""

target_y = df_y_test
y_pred = gbm.predict(df_x_test, num_iteration=gbm.best_iteration)

# print(y_pred)
print('df_y_test.shape: {}'.format(df_x_test.shape))
print('auc: {}'.format(roc_auc_score(target_y, y_pred)))
th = 0.5

# row: actually, column: predict, the smaller class will be the target for counting the following indexies
cm = confusion_matrix(target_y, (y_pred > th).astype(int), labels=[0,1])
print(cm)
print('precision_score: {}'.format(precision_score(target_y, (y_pred > th).astype(int))))
print('recall_score: {}'.format(recall_score(target_y, (y_pred > th).astype(int))))
print('f1_score: {}'.format(f1_score(target_y, (y_pred > th).astype(int))))
print('accuracy: {}'.format(accuracy_score(target_y, (y_pred > th).astype(int))))

"""# Data Processing scenario 9
- Base on scenerio 3
- Do normalization on numerical features
- Only use the feature that selected by subjective
  - use column : hh, mm, ss, locdt, stocn, hcefg, ecfg
- balance two class
- 64%(0.8 * 0.8) training data
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 
"""

import pandas as pd
with open('/content/drive/My Drive/fintech-introduction/term-project/train_feature_normalization.csv', 'r') as f:
  df = pd.read_csv(f)
  # df = df.set_index('txkey')


# print(df_X)
# print(df_y)
df

df_y = df['fraud_ind']
df_x = df[[ 'hh', 'locdt', 'stocn', 'hcefg', 'ecfg']]
df_x

# split train+val and test
df_x_data, df_x_test = train_test_split(df_x, test_size=0.2, shuffle=False)
df_y_data = df_y.loc[df_x_data.index]
df_y_test = df_y.loc[df_x_test.index]

# print(df_x_data)
# print(df_x_test)
# print(df_y_data)
# print(df_y_test)

#split train and val
df_x_train, df_x_val = train_test_split(df_x_data, test_size=0.2, shuffle=False)
df_y_train = df_y_data.loc[df_x_train.index]
df_y_val = df_y_data.loc[df_x_val.index]


print('df_x_train:')
print(df_x_train)
print('df_x_val:')
print(df_x_val)
print('df_x_test:')
print(df_x_test)
print('df_y_train:')
print(df_y_train)
print('df_y_val:')
print(df_y_val)
print('df_y_test:')
print(df_y_test)

# Balance two #example in two class
# df_y_train
df_fraud = df_x_train.loc[df_y_train == 1]
df_non_fraud = df_x_train.loc[df_y_train == 0].sample(n=df_fraud.shape[0])
df_x_train_balance = pd.concat([df_fraud, df_non_fraud])
df_y_train_balance = pd.concat([df_y_train.loc[df_fraud.index], df_y_train.loc[df_non_fraud.index]])
df_fraud, df_non_fraud, df_x_train_balance, df_y_train_balance

# create dataset for lightgbm
lgb_train = lgb.Dataset(df_x_train, df_y_train)
lgb_eval = lgb.Dataset(df_x_val, df_y_val)
print(lgb_train.data)
print(lgb_eval.data)

"""### Training

"""

# specify your configurations as a dict

lgb_param = {
    'boosting_type' : 'gbdt',
    'objective' : 'binary',
    'metric' : 'binary_logloss',
    'learning_rate' : 0.1,
}

print('Starting training...')
max_round = 100
gbm = lgb.train(lgb_param,
         lgb_train,
         num_boost_round=max_round,
         valid_sets=lgb_eval,
         early_stopping_rounds=max_round)

"""### Show the performance on testing set"""

target_y = df_y_test
y_pred = gbm.predict(df_x_test, num_iteration=gbm.best_iteration)

# print(y_pred)
print('df_y_test.shape: {}'.format(df_x_test.shape))
print('auc: {}'.format(roc_auc_score(target_y, y_pred)))
th = 0.5

# row: actually, column: predict, the smaller class will be the target for counting the following indexies
cm = confusion_matrix(target_y, (y_pred > th).astype(int), labels=[0,1])
print(cm)
print('precision_score: {}'.format(precision_score(target_y, (y_pred > th).astype(int))))
print('recall_score: {}'.format(recall_score(target_y, (y_pred > th).astype(int))))
print('f1_score: {}'.format(f1_score(target_y, (y_pred > th).astype(int))))
print('accuracy: {}'.format(accuracy_score(target_y, (y_pred > th).astype(int))))

"""# Data Processing scenario 10
- Base on scenerio 3
- Only use the feature that selected by subjective
  - use column : hh, mm, ss, locdt, stocn, hcefg, ecfg
- Do normalization on numerical features
- Convert catogorical featrues to one hot style
- Only about 400000 data in total
- 64%(0.8 * 0.8) training data
- 16%(0.8 * 0.2) to validate
- 20% to test
- f1 : 0.33
"""

import pandas as pd
with open('/content/drive/My Drive/fintech-introduction/term-project/train_feature_normalization_onehot_subjective.csv', 'r') as f:
  df = pd.read_csv(f)
  # df = df.set_index('txkey')


# print(df_X)
# print(df_y)
df

df_y = df['fraud_ind']
df_x = df.drop(['fraud_ind', 'Unnamed: 0'], 1)
df_x

# split train+val and test
df_x_data, df_x_test = train_test_split(df_x, test_size=0.2, shuffle=False)
df_y_data = df_y.loc[df_x_data.index]
df_y_test = df_y.loc[df_x_test.index]

# print(df_x_data)
# print(df_x_test)
# print(df_y_data)
# print(df_y_test)

#split train and val
df_x_train, df_x_val = train_test_split(df_x_data, test_size=0.2, shuffle=False)
df_y_train = df_y_data.loc[df_x_train.index]
df_y_val = df_y_data.loc[df_x_val.index]


print('df_x_train:')
print(df_x_train)
print('df_x_val:')
print(df_x_val)
print('df_x_test:')
print(df_x_test)
print('df_y_train:')
print(df_y_train)
print('df_y_val:')
print(df_y_val)
print('df_y_test:')
print(df_y_test)

# create dataset for lightgbm
lgb_train = lgb.Dataset(df_x_train, df_y_train)
lgb_eval = lgb.Dataset(df_x_val, df_y_val)
print(lgb_train.data)
print(lgb_eval.data)

"""### Training"""

# specify your configurations as a dict

lgb_param = {
    'boosting_type' : 'gbdt',
    'objective' : 'binary',
    'metric' : 'binary_logloss',
    'learning_rate' : 0.01,
}

print('Starting training...')
max_round = 1000
gbm = lgb.train(lgb_param,
         lgb_train,
         num_boost_round=max_round,
         valid_sets=lgb_eval,
         early_stopping_rounds=max_round)

"""### Show the performance on testing set"""

target_y = df_y_test
y_pred = gbm.predict(df_x_test, num_iteration=gbm.best_iteration)

# print(y_pred)
print('df_y_test.shape: {}'.format(df_x_test.shape))
print('auc: {}'.format(roc_auc_score(target_y, y_pred)))
th = 0.5

# row: actually, column: predict, the smaller class will be the target for counting the following indexies
cm = confusion_matrix(target_y, (y_pred > th).astype(int), labels=[0,1])
print(cm)
print('precision_score: {}'.format(precision_score(target_y, (y_pred > th).astype(int))))
print('recall_score: {}'.format(recall_score(target_y, (y_pred > th).astype(int))))
print('f1_score: {}'.format(f1_score(target_y, (y_pred > th).astype(int))))
print('accuracy: {}'.format(accuracy_score(target_y, (y_pred > th).astype(int))))